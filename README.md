# ğŸ•·ï¸ Go Web Crawler

A simple, concurrent web crawler written in Go that extracts all anchor links from a given domain. It recursively crawls only URLs that belong to the same domain, ensuring focused and efficient exploration.


## ğŸš€ Features

- ğŸŒ Extracts all anchor (`<a href="">`) links
- ğŸ”„ Recursively crawls internal links
- ğŸ”’ Skips SSL verification to support crawling HTTPS pages without valid certificates
- âš¡ Uses goroutines for concurrency
- ğŸ§  Avoids duplicate crawls with a visited URL map
- ğŸ§ª Built with `net/http`, `url`, and [steelx/extractlinks](https://github.com/steelx/extractlinks)

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/your-username/go-web-crawler.git
cd go-web-crawler
go mod tidy
```

---



## ğŸ§ª Usage

```bash
go run main.go https://example.com

